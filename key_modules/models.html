
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Models &#8212; CAMEL 0.2.22 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=3c972f42" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=30d83864"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'key_modules/models';</script>
    <link rel="icon" href="https://raw.githubusercontent.com/camel-ai/camel/master/misc/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Message" href="messages.html" />
    <link rel="prev" title="Data Generation" href="datagen.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="https://raw.githubusercontent.com/camel-ai/camel/master/misc/logo_light.png" class="logo__image only-light" alt=""/>
    <img src="https://raw.githubusercontent.com/camel-ai/camel/master/misc/logo_light.png" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">CAMEL 0.2.22</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/setup.html">API Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Key Modules</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="agents.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="datagen.html">Data Generation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="messages.html">Message</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory.html">Memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompts.html">Prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="tasks.html">Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="loaders.html">Loaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="storages.html">Storages</a></li>
<li class="toctree-l1"><a class="reference internal" href="society.html">Society</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="retrievers.html">Retrievers</a></li>
<li class="toctree-l1"><a class="reference internal" href="workforce.html">Workforce</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cookbooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../cookbooks/basic_concepts/index.html">Basic Concepts</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/basic_concepts/create_your_first_agent.html">Creating Your First Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/basic_concepts/create_your_first_agents_society.html">Creating Your First Agent Society</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/basic_concepts/agents_message.html">Message Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/basic_concepts/agents_prompting.html">Prompting Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/basic_concepts/model_speed_comparison.html">Model Speed Comparison Cookbook</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cookbooks/advanced_features/index.html">Advanced Features</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/agents_with_tools.html">Tools Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/agents_with_tools_from_Composio.html">Using Tools from Composio</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/agents_with_human_in_loop_and_tool_approval.html">Agents with Human-in-loop and Tool Approval from HumanLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/agents_with_memory.html">Memory Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/agents_with_rag.html">RAG Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/agents_with_graph_rag.html">Graph RAG Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/agents_tracking.html">Track CAMEL Agents with AgentOps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/critic_agents_and_tree_search.html">Critic Agents and Tree Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/advanced_features/embodied_agents.html">Embodied Agents</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cookbooks/multi_agent_society/index.html">Multi-agent Society</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/multi_agent_society/agents_society.html">Develop Trading Bot with Role Playing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/multi_agent_society/workforce_judge_committee.html">Create A Hackathon Judge Committee with Workforce</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/multi_agent_society/task_generation.html">Task Generation Cookbook</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cookbooks/data_generation/index.html">Agentic Data Generation</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/sft_data_generation_and_unsloth_finetuning_mistral_7b_instruct.html">Agenetic SFT Data generation with CAMEL and finetuning Mistral models with Unsloth</a></li>

<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/sft_data_generation_and_unsloth_finetuning_Qwen2_5_7B.html">Agenetic Data generation with CAMEL and finetuning Qwen models with Unsloth</a></li>

<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/sft_data_generation_and_unsloth_finetuning_tinyllama.html">Agenetic SFT Data generation with CAMEL and finetuning Meta models with Unsloth</a></li>

<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/data_gen_with_real_function_calls_and_hermes_format.html">Real Function Calls and Hermes Format Data Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/self_instruct_data_generation.html">Self-instruct Data Generation Using Qwen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/cot_data_gen_sft_qwen_unsolth_upload_huggingface.html">CoT Data Generation and SFT Qwen With Unsloth</a></li>


<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/synthetic_dataevaluation%26filter_with_reward_model.html">Agentic Data Generation, Evaluation &amp; Filtering with Reward Models</a></li>

<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/data_model_generation_and_structured_output_with_qwen.html">Agentic Data Model Generation and Structured Output Powered by CAMEL &amp; Qwen</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/distill_math_reasoning_data_from_deepseek_r1.html">Distill Math Reasoning Data from DeepSeek R1 with CAMEL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/self_improving_math_reasoning_data_distillation_from_deepSeek_r1.html">Self-Improving Math Reasoning Data Distillation from DeepSeek R1 with CAMEL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_generation/self_improving_cot_generation.html">Deep Dive into CAMELâ€™s Practices for Self-Improving CoT Generation ðŸš€</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cookbooks/applications/index.html">Applications</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/applications/roleplaying_scraper.html">Role-Playing Scraper for Report &amp; Knowledge Graph Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/applications/dynamic_travel_planner.html">Dynamic Travel Planner Role-Playing: Multi-Agent System with Real-Time Insights Powered by Dappier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/applications/customer_service_Discord_bot_with_agentic_RAG.html">Customer Service Discord Bot with Agentic RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/applications/customer_service_Discord_bot_using_SambaNova_with_agentic_RAG.html">Customer Service Discord Bot Using SambaNova with Agentic RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/applications/customer_service_Discord_bot_using_local_model_with_agentic_RAG.html">Customer Service Discord Bot Using Local Models with Agentic RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/applications/finance_discord_bot.html">Customer Service Discord Bot for Finance with OpenBB</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cookbooks/data_processing/index.html">Data Processing and Analysis</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_processing/video_analysis.html">Video Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_processing/agent_with_chunkr_for_pdf_parsing.html">Create AI Agents that work with your PDFs using Chunkr &amp; Mistral AI</a></li>

<li class="toctree-l2"><a class="reference internal" href="../cookbooks/data_processing/ingest_data_from_websites_with_Firecrawl.html">3 ways to ingest data from websites with Firecrawl</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../modules.html">camel</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../camel.html">camel package</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../camel.agents.html">camel.agents package</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../camel.agents.tool_agents.html">camel.agents.tool_agents package</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.configs.html">camel.configs package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.embeddings.html">camel.embeddings package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.interpreters.html">camel.interpreters package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.loaders.html">camel.loaders package</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../camel.memories.html">camel.memories package</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../camel.memories.blocks.html">camel.memories.blocks package</a></li>
<li class="toctree-l4"><a class="reference internal" href="../camel.memories.context_creators.html">camel.memories.context_creators package</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.messages.html">camel.messages package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.models.html">camel.models package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.prompts.html">camel.prompts package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.responses.html">camel.responses package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.retrievers.html">camel.retrievers package</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../camel.societies.html">camel.societies package</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../camel.societies.workforce.html">camel.societies.workforce package</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../camel.storages.html">camel.storages package</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../camel.storages.graph_storages.html">camel.storages.graph_storages package</a></li>
<li class="toctree-l4"><a class="reference internal" href="../camel.storages.key_value_storages.html">camel.storages.key_value_storages package</a></li>
<li class="toctree-l4"><a class="reference internal" href="../camel.storages.object_storages.html">camel.storages.object_storages package</a></li>
<li class="toctree-l4"><a class="reference internal" href="../camel.storages.vectordb_storages.html">camel.storages.vectordb_storages package</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.tasks.html">camel.tasks package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.terminators.html">camel.terminators package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.toolkits.html">camel.toolkits package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.types.html">camel.types package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../camel.utils.html">camel.utils package</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/camel-ai/camel" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/camel-ai/camel/edit/main/key_modules/models.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/camel-ai/camel/issues/new?title=Issue%20on%20page%20%2Fkey_modules/models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/key_modules/models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> On this page </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concept">1. Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-model-platforms">2. Supported Model Platforms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-models-by-api-calling">3. Using Models by API calling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-on-device-open-source-models">4. Using On-Device Open Source Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ollama-to-set-llama-3-locally">4.1 Using Ollama to Set Llama 3 Locally</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-vllm-to-set-phi-3-locally">4.2 Using vLLM to Set Phi-3 Locally</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-sglang-to-set-meta-llama-llama-locally">4.3 Using SGLang to Set meta-llama/Llama Locally</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#about-model-speed">5. About Model Speed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">6. Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="models">
<h1>Models<a class="headerlink" href="#models" title="Link to this heading">#</a></h1>
<section id="concept">
<h2>1. Concept<a class="headerlink" href="#concept" title="Link to this heading">#</a></h2>
<p>The model is the brain of the intelligent agent, responsible for processing all input and output data. By calling different models, the agent can execute operations such as text analysis, image recognition, or complex reasoning according to task requirements. CAMEL offers a range of standard and customizable interfaces, as well as seamless integrations with various components, to facilitate the development of applications with Large Language Models (LLMs). In this part, we will introduce models currently supported by CAMEL and the working principles and interaction methods with models.</p>
<p>All the codes are also available on colab notebook <a class="reference external" href="https://colab.research.google.com/drive/18hQLpte6WW2Ja3Yfj09NRiVY-6S2MFu7?usp=sharing">here</a>.</p>
</section>
<section id="supported-model-platforms">
<h2>2. Supported Model Platforms<a class="headerlink" href="#supported-model-platforms" title="Link to this heading">#</a></h2>
<p>The following table lists currently supported model platforms by CAMEL.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model Platform</p></th>
<th class="head"><p>Available Models</p></th>
<th class="head"><p>Multi-modality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>OpenAI</p></td>
<td><p>gpt-4o</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>OpenAI</p></td>
<td><p>gpt-4o-mini</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>OpenAI</p></td>
<td><p>o1</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>OpenAI</p></td>
<td><p>o1-preview</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>OpenAI</p></td>
<td><p>o1-mini</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>OpenAI</p></td>
<td><p>gpt-4-turbo</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>OpenAI</p></td>
<td><p>gpt-4</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>OpenAI</p></td>
<td><p>gpt-3.5-turbo</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Azure OpenAI</p></td>
<td><p>gpt-4o</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Azure OpenAI</p></td>
<td><p>gpt-4-turbo</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Azure OpenAI</p></td>
<td><p>gpt-4</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Azure OpenAI</p></td>
<td><p>gpt-3.5-turbo</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>OpenAI Compatible</p></td>
<td><p>Depends on the provider</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-odd"><td><p>Mistral AI</p></td>
<td><p>mistral-large-2</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Mistral AI</p></td>
<td><p>pixtral-12b-2409</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Mistral AI</p></td>
<td><p>ministral-8b-latest</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Mistral AI</p></td>
<td><p>ministral-3b-latest</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Mistral AI</p></td>
<td><p>open-mistral-nemo</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Mistral AI</p></td>
<td><p>codestral</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Mistral AI</p></td>
<td><p>open-mistral-7b</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Mistral AI</p></td>
<td><p>open-mixtral-8x7b</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Mistral AI</p></td>
<td><p>open-mixtral-8x22b</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Mistral AI</p></td>
<td><p>open-codestral-mamba</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Moonshot</p></td>
<td><p>moonshot-v1-8k</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Moonshot</p></td>
<td><p>moonshot-v1-32k</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Moonshot</p></td>
<td><p>moonshot-v1-128k</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Anthropic</p></td>
<td><p>claude-3-5-sonnet-latest</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Anthropic</p></td>
<td><p>claude-3-5-haiku-latest</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Anthropic</p></td>
<td><p>claude-3-haiku-20240307</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Anthropic</p></td>
<td><p>claude-3-sonnet-20240229</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Anthropic</p></td>
<td><p>claude-3-opus-latest</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Anthropic</p></td>
<td><p>claude-2.0</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Gemini</p></td>
<td><p>gemini-2.0-flash-exp</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Gemini</p></td>
<td><p>gemini-1.5-pro</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Gemini</p></td>
<td><p>gemini-1.5-flash</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Gemini</p></td>
<td><p>gemini-exp-1114</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Lingyiwanwu</p></td>
<td><p>yi-lightning</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Lingyiwanwu</p></td>
<td><p>yi-large</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Lingyiwanwu</p></td>
<td><p>yi-medium</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Lingyiwanwu</p></td>
<td><p>yi-large-turbo</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Lingyiwanwu</p></td>
<td><p>yi-vision</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Lingyiwanwu</p></td>
<td><p>yi-medium-200k</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Lingyiwanwu</p></td>
<td><p>yi-spark</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Lingyiwanwu</p></td>
<td><p>yi-large-rag</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Lingyiwanwu</p></td>
<td><p>yi-large-fc</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen</p></td>
<td><p>qwq-32b-preview</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Qwen</p></td>
<td><p>qwen-max</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen</p></td>
<td><p>qwen-plus</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Qwen</p></td>
<td><p>qwen-turbo</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen</p></td>
<td><p>qwen-long</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Qwen</p></td>
<td><p>qwen-vl-max</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen</p></td>
<td><p>qwen-vl-plus</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Qwen</p></td>
<td><p>qwen-math-plus</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen</p></td>
<td><p>qwen-math-turbo</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Qwen</p></td>
<td><p>qwen-coder-turbo</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen</p></td>
<td><p>qwen2.5-coder-32b-instruct</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Qwen</p></td>
<td><p>qwen2.5-72b-instruct</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen</p></td>
<td><p>qwen2.5-32b-instruct</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Qwen</p></td>
<td><p>qwen2.5-14b-instruct</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>DeepSeek</p></td>
<td><p>deepseek-chat</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>DeepSeek</p></td>
<td><p>deepseek-reasoner</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>ZhipuAI</p></td>
<td><p>glm-4</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>ZhipuAI</p></td>
<td><p>glm-4v</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>ZhipuAI</p></td>
<td><p>glm-4v-flash</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>ZhipuAI</p></td>
<td><p>glm-4v-plus-0111</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>ZhipuAI</p></td>
<td><p>glm-4-plus</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>ZhipuAI</p></td>
<td><p>glm-4-air</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>ZhipuAI</p></td>
<td><p>glm-4-air-0111</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>ZhipuAI</p></td>
<td><p>glm-4-airx</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>ZhipuAI</p></td>
<td><p>glm-4-long</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>ZhipuAI</p></td>
<td><p>glm-4-flashx</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>ZhipuAI</p></td>
<td><p>glm-zero-preview</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>ZhipuAI</p></td>
<td><p>glm-4-flash</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>ZhipuAI</p></td>
<td><p>glm-3-turbo</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>InternLM</p></td>
<td><p>internlm3-latest</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>InternLM</p></td>
<td><p>internlm3-8b-instruct</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>InternLM</p></td>
<td><p>internlm2.5-latest</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>InternLM</p></td>
<td><p>internlm2-pro-chat</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>Reka</p></td>
<td><p>reka-core</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Reka</p></td>
<td><p>reka-flash</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Reka</p></td>
<td><p>reka-edge</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Nvidia</p></td>
<td><p>https://docs.api.nvidia.com/nim/reference/llm-apis</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-even"><td><p>SambaNova</p></td>
<td><p>https://community.sambanova.ai/t/supported-models/193</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-odd"><td><p>Groq</p></td>
<td><p>https://console.groq.com/docs/models</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-even"><td><p>Ollama</p></td>
<td><p>https://ollama.com/library</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-odd"><td><p>vLLM</p></td>
<td><p>https://docs.vllm.ai/en/latest/models/supported_models.html</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-even"><td><p>Together AI</p></td>
<td><p>https://docs.together.ai/docs/chat-models</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-odd"><td><p>LiteLLM</p></td>
<td><p>https://docs.litellm.ai/docs/providers</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-even"><td><p>SGLang</p></td>
<td><p>https://sgl-project.github.io/references/supported_models.html</p></td>
<td><p>â€”â€“</p></td>
</tr>
<tr class="row-odd"><td><p>AIML</p></td>
<td><p>https://docs.aimlapi.com/api-overview/model-database/text-models</p></td>
<td><p>â€”â€“</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="using-models-by-api-calling">
<h2>3. Using Models by API calling<a class="headerlink" href="#using-models-by-api-calling" title="Link to this heading">#</a></h2>
<p>Here is an example code to use a specific model (gpt-4o-mini). If you want to use another model, you can simply change these three parameters: <code class="docutils literal notranslate"><span class="pre">model_platform</span></code>, <code class="docutils literal notranslate"><span class="pre">model_type</span></code>, <code class="docutils literal notranslate"><span class="pre">model_config_dict</span></code> .</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">camel.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelFactory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelPlatformType</span><span class="p">,</span> <span class="n">ModelType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.configs</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatGPTConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.agents</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAgent</span>

<span class="c1"># Define the model, here in this case we use gpt-4o-mini</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ModelFactory</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model_platform</span><span class="o">=</span><span class="n">ModelPlatformType</span><span class="o">.</span><span class="n">OPENAI</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="n">ModelType</span><span class="o">.</span><span class="n">GPT_4O_MINI</span><span class="p">,</span>
    <span class="n">model_config_dict</span><span class="o">=</span><span class="n">ChatGPTConfig</span><span class="p">()</span><span class="o">.</span><span class="n">as_dict</span><span class="p">(),</span>
<span class="p">)</span>

<span class="c1"># Define an assitant message</span>
<span class="n">system_msg</span> <span class="o">=</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span>

<span class="c1"># Initialize the agent</span>
<span class="n">ChatAgent</span><span class="p">(</span><span class="n">system_msg</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>And if you want to use an OpenAI-compatible API, you can replace the <code class="docutils literal notranslate"><span class="pre">model</span></code> with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ModelFactory</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model_platform</span><span class="o">=</span><span class="n">ModelPlatformType</span><span class="o">.</span><span class="n">OPENAI_COMPATIBLE_MODEL</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;a-string-representing-the-model-type&quot;</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_COMPATIBILIY_API_KEY&quot;</span><span class="p">),</span>
    <span class="n">url</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_COMPATIBILIY_API_BASE_URL&quot;</span><span class="p">),</span>
    <span class="n">model_config_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-on-device-open-source-models">
<h2>4. Using On-Device Open Source Models<a class="headerlink" href="#using-on-device-open-source-models" title="Link to this heading">#</a></h2>
<p>In the current landscape, for those seeking highly stable content generation, OpenAIâ€™s gpt-4o-mini, gpt-4o are often recommended. However, the field is rich with many other outstanding open-source models that also yield commendable results. CAMEL can support developers to delve into integrating these open-source large language models (LLMs) to achieve project outputs based on unique input ideas.</p>
<section id="using-ollama-to-set-llama-3-locally">
<h3>4.1 Using Ollama to Set Llama 3 Locally<a class="headerlink" href="#using-ollama-to-set-llama-3-locally" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Download <a class="reference external" href="https://ollama.com/download">Ollama</a>.</p></li>
<li><p>After setting up Ollama, pull the Llama3 model by typing the following command into the terminal:</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>pull<span class="w"> </span>llama3
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">ModelFile</span></code> similar the one below in your project directory. (Optional)</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FROM</span> <span class="n">llama3</span>

<span class="c1"># Set parameters</span>
<span class="n">PARAMETER</span> <span class="n">temperature</span> <span class="mf">0.8</span>
<span class="n">PARAMETER</span> <span class="n">stop</span> <span class="n">Result</span>

<span class="c1"># Sets a custom system message to specify the behavior of the chat assistant</span>
<span class="c1"># Leaving it blank for now.</span>

<span class="n">SYSTEM</span> <span class="s2">&quot;&quot;&quot; &quot;&quot;&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Create a script to get the base model (llama3) and create a custom model using the <code class="docutils literal notranslate"><span class="pre">ModelFile</span></code> above. Save this as a <code class="docutils literal notranslate"><span class="pre">.sh</span></code> file: (Optional)</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/zsh

# variables
model_name=&quot;llama3&quot;
custom_model_name=&quot;camel-llama3&quot;

#get the base model
ollama pull $model_name

#create the model file
ollama create $custom_model_name -f ./Llama3ModelFile
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Navigate to the directory where the script and <code class="docutils literal notranslate"><span class="pre">ModelFile</span></code> are located and run the script. Enjoy your Llama3 model, enhanced by CAMELâ€™s excellent agents.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">camel.agents</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAgent</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelFactory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelPlatformType</span>

<span class="n">ollama_model</span> <span class="o">=</span> <span class="n">ModelFactory</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model_platform</span><span class="o">=</span><span class="n">ModelPlatformType</span><span class="o">.</span><span class="n">OLLAMA</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span>
    <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://localhost:11434/v1&quot;</span><span class="p">,</span> <span class="c1"># Optional</span>
    <span class="n">model_config_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">},</span>
<span class="p">)</span>

<span class="n">agent_sys_msg</span> <span class="o">=</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">ChatAgent</span><span class="p">(</span><span class="n">agent_sys_msg</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">ollama_model</span><span class="p">,</span> <span class="n">token_limit</span><span class="o">=</span><span class="mi">4096</span><span class="p">)</span>

<span class="n">user_msg</span> <span class="o">=</span> <span class="s2">&quot;Say hi to CAMEL&quot;</span>

<span class="n">assistant_response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">user_msg</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">assistant_response</span><span class="o">.</span><span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-vllm-to-set-phi-3-locally">
<h3>4.2 Using vLLM to Set Phi-3 Locally<a class="headerlink" href="#using-vllm-to-set-phi-3-locally" title="Link to this heading">#</a></h3>
<p>Install <a class="reference external" href="https://docs.vllm.ai/en/latest/getting_started/installation.html">vLLM</a> first.</p>
<p>After setting up vLLM, start an OpenAI compatible server for example by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">vllm</span><span class="o">.</span><span class="n">entrypoints</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">api_server</span> <span class="o">--</span><span class="n">model</span> <span class="n">microsoft</span><span class="o">/</span><span class="n">Phi</span><span class="o">-</span><span class="mi">3</span><span class="o">-</span><span class="n">mini</span><span class="o">-</span><span class="mi">4</span><span class="n">k</span><span class="o">-</span><span class="n">instruct</span> <span class="o">--</span><span class="n">api</span><span class="o">-</span><span class="n">key</span> <span class="n">vllm</span> <span class="o">--</span><span class="n">dtype</span> <span class="n">bfloat16</span>
</pre></div>
</div>
<p>Create and run following script (more details please refer to this <a class="reference external" href="https://github.com/camel-ai/camel/blob/master/examples/models/vllm_model_example.py">example</a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">camel.agents</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAgent</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelFactory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelPlatformType</span>

<span class="n">vllm_model</span> <span class="o">=</span> <span class="n">ModelFactory</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model_platform</span><span class="o">=</span><span class="n">ModelPlatformType</span><span class="o">.</span><span class="n">VLLM</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span><span class="p">,</span>
    <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span> <span class="c1"># Optional</span>
    <span class="n">model_config_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span> <span class="c1"># Optional</span>
<span class="p">)</span>

<span class="n">agent_sys_msg</span> <span class="o">=</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">ChatAgent</span><span class="p">(</span><span class="n">agent_sys_msg</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">vllm_model</span><span class="p">,</span> <span class="n">token_limit</span><span class="o">=</span><span class="mi">4096</span><span class="p">)</span>

<span class="n">user_msg</span> <span class="o">=</span> <span class="s2">&quot;Say hi to CAMEL AI&quot;</span>

<span class="n">assistant_response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">user_msg</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">assistant_response</span><span class="o">.</span><span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-sglang-to-set-meta-llama-llama-locally">
<h3>4.3 Using SGLang to Set meta-llama/Llama Locally<a class="headerlink" href="#using-sglang-to-set-meta-llama-llama-locally" title="Link to this heading">#</a></h3>
<p>Install <a class="reference external" href="https://sgl-project.github.io/start/install.html">SGLang</a> first.</p>
<p>Create and run following script (more details please refer to this <a class="reference external" href="https://github.com/camel-ai/camel/blob/master/examples/models/sglang_model_example.py">example</a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">camel.agents</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatAgent</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelFactory</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">camel.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelPlatformType</span>

<span class="n">sglang_model</span> <span class="o">=</span> <span class="n">ModelFactory</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model_platform</span><span class="o">=</span><span class="n">ModelPlatformType</span><span class="o">.</span><span class="n">SGLANG</span><span class="p">,</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">model_config_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">},</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;sglang&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">agent_sys_msg</span> <span class="o">=</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">ChatAgent</span><span class="p">(</span><span class="n">agent_sys_msg</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">sglang_model</span><span class="p">,</span> <span class="n">token_limit</span><span class="o">=</span><span class="mi">4096</span><span class="p">)</span>

<span class="n">user_msg</span> <span class="o">=</span> <span class="s2">&quot;Say hi to CAMEL AI&quot;</span>

<span class="n">assistant_response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">user_msg</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">assistant_response</span><span class="o">.</span><span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="about-model-speed">
<h2>5. About Model Speed<a class="headerlink" href="#about-model-speed" title="Link to this heading">#</a></h2>
<p>Model speed is a crucial factor in AI application performance. It affects both user experience and system efficiency, especially in real-time or interactive tasks. In <a class="reference internal" href="#../cookbooks/model_speed_comparison.ipynb"><span class="xref myst">this notebook</span></a>, we compared several models, including OpenAIâ€™s GPT-4O Mini, GPT-4O, O1 Preview, and SambaNovaâ€™s Llama series, by measuring the number of tokens each model processes per second.</p>
<p>Key Insights:
Smaller models like SambaNovaâ€™s Llama 8B and OpenAIâ€™s GPT-4O Mini typically offer faster responses.
Larger models like SambaNovaâ€™s Llama 405B, while more powerful, tend to generate output more slowly due to their complexity.
OpenAI models demonstrate relatively consistent performance, while SambaNovaâ€™s Llama 8B significantly outperforms others in speed.
The chart below illustrates the tokens per second achieved by each model during our tests:</p>
<p><img alt="Model Speed Comparison" src="https://i.postimg.cc/4xByytyZ/model-speed.png" /></p>
<p>For local inference, we conducted a straightforward comparison locally between vLLM and SGLang. SGLang demonstrated superior performance, with <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-1B-Instruct</span></code> reaching a peak speed of 220.98 tokens per second, compared to vLLM, which capped at 107.2 tokens per second.</p>
</section>
<section id="conclusion">
<h2>6. Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In conclusion, CAMEL empowers developers to explore and integrate these diverse models, unlocking new possibilities for innovative AI applications. The world of large language models offers a rich tapestry of options beyond just the well-known proprietary solutions. By guiding users through model selection, environment setup, and integration, CAMEL bridges the gap between cutting-edge AI research and practical implementation. Its hybrid approach, combining in-house implementations with third-party integrations, offers unparalleled flexibility and comprehensive support for LLM-based development. Donâ€™t just watch this transformation that is happening from the sidelines.</p>
<p>Dive into the CAMEL documentation, experiment with different models, and be part of shaping the future of AI. The era of truly flexible and powerful AI is here - are you ready to make your mark?</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="datagen.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Data Generation</p>
      </div>
    </a>
    <a class="right-next"
       href="messages.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Message</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concept">1. Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-model-platforms">2. Supported Model Platforms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-models-by-api-calling">3. Using Models by API calling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-on-device-open-source-models">4. Using On-Device Open Source Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ollama-to-set-llama-3-locally">4.1 Using Ollama to Set Llama 3 Locally</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-vllm-to-set-phi-3-locally">4.2 Using vLLM to Set Phi-3 Locally</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-sglang-to-set-meta-llama-llama-locally">4.3 Using SGLang to Set meta-llama/Llama Locally</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#about-model-speed">5. About Model Speed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">6. Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By CAMEL-AI.org
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2024, CAMEL-AI.org.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>